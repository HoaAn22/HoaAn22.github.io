# Giới thiệu về Gradient Descent

**Gradient Descent** là một thuật toán tối ưu quen thuộc trong lĩnh vực Machine, là thuật toán lặp để tìm cực trị của 1 hàm khả vi. Thường được ứng dụng làm **hàm mất mát/lỗi (loss function)** cho mô hình học máy để tối ưu độ chính xác mô hình.

**Ý nghĩa của thuật toán này đối với mô hình học**:
* Các thuật toán tối ưu nói chung và thuật toán Gradient Descent nói riêng là một thứ vô cùng quan trọng trong quá trình train mô hình machine learning và deep learning, giúp mô hình tăng độ chính xác tối ưu các siêu tham số, học được phân phân phối và ý nghĩa của dữ liệu.
* Đối với các mô hình như Deep learning phức tạp, phải xử lý dữ liệu ma trận nhiều, việc tối ưu được các siêu tham số cho mô hình này vô cùng khó gần như không thể với các phương pháp truyền thống. Ứng dụng thuật toán này chúng ta sẽ tìm được tham số sấp sỉ **(được xem là kết quả)**. Các siêu tham số của mô hình sẽ dựa trên **tốc độ học (learning rate)** tốc độ càng nhỏ các siêu tham số càng sát với dữ liệu.

Tuy nhiên, tốc độ học quá nhỏ cũng không hẳn là tốt khi train mô hình. Chúng ta sẽ tìm hiểu về vấn đề này ở các bài học sau
# Ý tưởng của thuật toán



# Bài mẫu & Code
[Google Colab - Gradient Descent](https://colab.research.google.com/drive/1OjctxSOZrDB4vTevji3xCcYy5lkpTyAK#scrollTo=r4FOwJlggSk3)

# Tài liệu tham khảo

# Nguồn ảnh

